1. Get the application URL by running these commands:
{{- if .Values.ingress.enabled }}
{{- range $host := .Values.ingress.hosts }}
  {{- range .paths }}
  http{{ if $.Values.ingress.tls }}s{{ end }}://{{ $host.host }}{{ .path }}
  {{- end }}
{{- end }}
{{- else if contains "NodePort" .Values.service.type }}
  export NODE_PORT=$(kubectl get --namespace {{ .Release.Namespace }} -o jsonpath="{.spec.ports[0].nodePort}" services {{ include "torchserve.fullname" . }})
  export NODE_IP=$(kubectl get nodes --namespace {{ .Release.Namespace }} -o jsonpath="{.items[0].status.addresses[0].address}")
  echo http://$NODE_IP:$NODE_PORT
{{- else if contains "LoadBalancer" .Values.service.type }}
     NOTE: It may take a few minutes for the LoadBalancer IP to be available.
           You can watch the status of by running 'kubectl get --namespace {{ .Release.Namespace }} svc -w {{ include "torchserve.fullname" . }}'
  export SERVICE_IP=$(kubectl get svc --namespace {{ .Release.Namespace }} {{ include "torchserve.fullname" . }} --template "{{"{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}"}}")
  echo http://$SERVICE_IP:{{ .Values.service.inference.port }}
{{- else if contains "ClusterIP" .Values.service.type }}
  export POD_NAME=$(kubectl get pods --namespace {{ .Release.Namespace }} -l "app.kubernetes.io/name={{ include "torchserve.name" . }},app.kubernetes.io/instance={{ .Release.Name }}" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace {{ .Release.Namespace }} $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace {{ .Release.Namespace }} port-forward $POD_NAME 8080:$CONTAINER_PORT
{{- end }}

2. TorchServe Endpoints:
   - Inference API: http://{{ include "torchserve.fullname" . }}:{{ .Values.service.inference.port }}
   - Management API: http://{{ include "torchserve.fullname" . }}:{{ .Values.service.management.port }}
   {{- if .Values.metrics.enabled }}
   - Metrics API: http://{{ include "torchserve.fullname" . }}:{{ .Values.service.metrics.port }}/metrics
   {{- end }}

3. Test the inference endpoint:
   kubectl run --namespace {{ .Release.Namespace }} torchserve-client --rm -it --restart=Never --image=curlimages/curl -- curl http://{{ include "torchserve.fullname" . }}:{{ .Values.service.inference.port }}/ping

4. View available models:
   kubectl run --namespace {{ .Release.Namespace }} torchserve-client --rm -it --restart=Never --image=curlimages/curl -- curl http://{{ include "torchserve.fullname" . }}:{{ .Values.service.management.port }}/models

{{- if .Values.torchserve.models }}

5. Available Models:
{{- range $name, $config := .Values.torchserve.models }}
   - {{ $name }}
{{- end }}

6. Example inference request:
   kubectl run --namespace {{ .Release.Namespace }} torchserve-client --rm -it --restart=Never --image=curlimages/curl -- curl -X POST http://{{ include "torchserve.fullname" . }}:{{ .Values.service.inference.port }}/predictions/{{ (keys .Values.torchserve.models | first) }} -T your_input_file
{{- end }}

7. Access logs:
   kubectl logs --namespace {{ .Release.Namespace }} -l "app.kubernetes.io/name={{ include "torchserve.name" . }},app.kubernetes.io/instance={{ .Release.Name }}"

{{- if .Values.persistence.enabled }}

8. Model Store Persistence:
   Your models are persisted using PVC: {{ include "torchserve.modelStorePvcName" . }}
   Mount path: {{ .Values.persistence.mountPath }}
{{- end }}

{{- if .Values.metrics.serviceMonitor.enabled }}

9. Monitoring:
   ServiceMonitor has been created for Prometheus scraping.
   Metrics are available at: http://{{ include "torchserve.fullname" . }}:{{ .Values.service.metrics.port }}/metrics
{{- end }}
